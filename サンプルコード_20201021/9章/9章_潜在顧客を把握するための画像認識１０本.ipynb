{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8O_38K85ij3j"
   },
   "outputs": [],
   "source": [
    "# 警告(worning)の非表示化\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\yujit\\anaconda3\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\yujit\\anaconda3\\lib\\site-packages (from opencv-contrib-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jbQ8XRVij3k"
   },
   "source": [
    "# 9章 潜在顧客を把握するための画像認識１０本\n",
    "\n",
    "ここでは、カメラから取得した映像を用いて画像認識を行い、  \n",
    "必要な情報を取得するための流れを学ぶことで、  \n",
    "画像認識をビジネス現場で応用するイメージをつかみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnxdBp8Pij3l"
   },
   "source": [
    "### ノック８１：画像データを読み込んでみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "reP86-_Sij3l",
    "outputId": "b4a3b9e6-ac37-4bf5-fb4c-101fb66c845e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "画像幅: 1920\n",
      "画像高さ: 1440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "# 画像読み込み\n",
    "img = cv2.imread(\"img/img01.jpg\")\n",
    "# img.shape: 行数,列数,チャンネル数(色相数)\n",
    "height, width = img.shape[:2]\n",
    "print(\"画像幅: \" + str(width))\n",
    "print(\"画像高さ: \" + str(height))\n",
    "# 画像の表示\n",
    "cv2.imshow(\"img\",img)\n",
    "# google_colabではopenCVの代わりにcv2_imshowを使用する\n",
    "# from google.colab.patches import cv2_imshow\n",
    "# cv2_imshow(img)\n",
    "\n",
    "# 画像を表示し続けるミリ秒数(ウィンドウを閉じるまでなら0を指定)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RXHi1jEij3m"
   },
   "source": [
    "### ノック８２：映像データを読み込んでみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ygv7WWdEij3m",
    "outputId": "97ccd754-e0ef-4a1d-da3a-d647e18a4c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "画像幅: 1920.0\n",
      "画像高さ: 1440.0\n",
      "総フレーム数: 401.0\n",
      "FPS: 30.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# 情報取得 #\n",
    "# 映像読み込み\n",
    "cap = cv2.VideoCapture(\"mov/mov01.avi\")\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) #幅\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) #高さ\n",
    "count = cap.get(cv2.CAP_PROP_FRAME_COUNT) #総フレーム数\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) #FPS\n",
    "print(\"画像幅: \" + str(width))\n",
    "print(\"画像高さ: \" + str(height))\n",
    "print(\"総フレーム数: \" + str(count))\n",
    "print(\"FPS: \" + str(fps))\n",
    "\n",
    "# 出力 #\n",
    "while(cap.isOpened()):\n",
    "    # frame: 画像情報として扱える\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"frame\", frame)\n",
    "    # 全フレームの処理終了or[q]キーで終了\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "# 後処理\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u33mA7sUij3n"
   },
   "source": [
    "### ノック８３：映像を画像に分割し、保存してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfSJ77JKij3n"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(\"mov/mov01.avi\")\n",
    "num = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"frame\", frame)\n",
    "        filepath = \"snapshot/snapshot_\" + str(num) + \".jpg\"\n",
    "        # 画像書き出し\n",
    "        cv2.imwrite(filepath,frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    num = num + 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0v2owrYij3n"
   },
   "source": [
    "### ノック８４：画像内のどこに人がいるのかを検出してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XZX3fpVAij3n",
    "outputId": "30944033-9dea-4dc6-8de4-f81d5ce4458a"
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'detectMultiScale'\n> Overload resolution failed:\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 人の検出(位置情報はhumanに格納される)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m human, r \u001b[38;5;241m=\u001b[39m hog\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhogParams)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(human)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m human:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# 画像に四角形を描く\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'detectMultiScale'\n> Overload resolution failed:\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# 準備 #\n",
    "# HOG特徴量: Histogram of Oriented Gradients(輝度勾配)\n",
    "# ヒトのシルエットを見て、形の特徴を位置や角度で表現するもの\n",
    "# 人の認識を簡単に行うために使用\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "# ヒトのモデル情報セット\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0, 'finalThreshold':5}\n",
    "\n",
    "# 検出 #\n",
    "img = cv2.imread(\"img/img01.jpg\")\n",
    "# 画像のモノクロ化\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# 人の検出(位置情報はhumanに格納される)\n",
    "human, r = hog.detectMultiScale(gray, **hogParams)\n",
    "\n",
    "if (len(human)>0):\n",
    "    for (x, y, w, h) in human:\n",
    "        # 画像に四角形を描く\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255,255,255), 3)\n",
    "\n",
    "# cv2.imshow(\"img\",img)\n",
    "cv2_imshow(img)\n",
    "cv2.imwrite(\"temp.jpg\",img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK99r_Rjij3n"
   },
   "source": [
    "### ノック８５：画像内の人の顔を検出してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H7P4IuLfij3o",
    "outputId": "7efbe660-b14a-4251-b74a-1b471b09b906"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# 準備\n",
    "# 正面顔を認識するモデル(横顔や目,鼻などもある)\n",
    "cascade_file = \"haarcascade_frontalface_alt.xml\"\n",
    "# 顔検出にはCascadeClassifierが伝統的に使われている\n",
    "# 顔検出や個人認証の分野には長い歴史があるため、深掘りして性能比較をしても良いかも\n",
    "cascade = cv2.CascadeClassifier(cascade_file)\n",
    "\n",
    "# 検出\n",
    "img = cv2.imread(\"img/img02.jpg\")\n",
    "# グレースケール\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# 検出の実行\n",
    "face_list = cascade.detectMultiScale(gray, minSize=(50, 50))\n",
    "\n",
    "# 検出した顔に印を付ける\n",
    "for (x, y, w, h) in face_list:\n",
    "    color = (0, 0, 225)\n",
    "    pen_w = 3\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), color, thickness = pen_w)\n",
    "\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.imwrite(\"temp.jpg\",img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsDAsKkxij3o"
   },
   "source": [
    "### ノック８６：画像内の人がどこに顔を向けているのかを検出してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dlib\n",
      "  Downloading dlib-19.24.1.tar.gz (3.2 MB)\n",
      "     ---------------------------------------- 3.2/3.2 MB 7.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: dlib\n",
      "  Building wheel for dlib (setup.py): started\n",
      "  Building wheel for dlib (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for dlib\n",
      "Failed to build dlib\n",
      "Installing collected packages: dlib\n",
      "  Running setup.py install for dlib: started\n",
      "  Running setup.py install for dlib: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [7 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  running build_ext\n",
      "  \n",
      "  ERROR: CMake must be installed to build dlib\n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for dlib\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for dlib did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [9 lines of output]\n",
      "  running install\n",
      "  C:\\Users\\yujit\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  running build_ext\n",
      "  \n",
      "  ERROR: CMake must be installed to build dlib\n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "dlib\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JROBEP_eij3o",
    "outputId": "2318bedd-f683-4105-f959-053a0715cd59"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 顔器官(フェイス・ランドマーク)と言われる目・鼻・口・輪郭を68の特徴点で表現することができる\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 人がどこに顔を向けているか等の細かな情報を検出できる\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 具体的には、輪郭の重心と内側の重心の差から顔の方位を割り出している\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 詳しくは専門書を読む\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 準備 #\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 68個の顔器官のモデルの読み込み\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# 顔器官(フェイス・ランドマーク)と言われる目・鼻・口・輪郭を68の特徴点で表現することができる\n",
    "# 人がどこに顔を向けているか等の細かな情報を検出できる\n",
    "# 具体的には、輪郭の重心と内側の重心の差から顔の方位を割り出している\n",
    "# 詳しくは専門書を読む\n",
    "import dlib\n",
    "import math\n",
    "\n",
    "# 準備 #\n",
    "# 68個の顔器官のモデルの読み込み\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "# 正面顔のモデルの読み込み\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# 検出 #\n",
    "img = cv2.imread(\"img/img02.jpg\")\n",
    "dets = detector(img, 1)\n",
    "\n",
    "for k, d in enumerate(dets):\n",
    "    shape = predictor(img, d)\n",
    "\n",
    "    # 顔領域の表示\n",
    "    color_f = (0, 0, 225)\n",
    "    color_l_out = (255, 0, 0)\n",
    "    color_l_in = (0, 255, 0)\n",
    "    line_w = 3\n",
    "    circle_r = 3\n",
    "    fontType = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontSize = 1\n",
    "    cv2.rectangle(img, (d.left(), d.top()), (d.right(), d.bottom()), color_f, line_w)\n",
    "    cv2.putText(img, str(k), (d.left(), d.top()), fontType, fontSize, color_f, line_w)\n",
    "\n",
    "    # 重心を導出する箱を用意\n",
    "    num_of_points_out = 17\n",
    "    num_of_points_in = shape.num_parts - num_of_points_out\n",
    "    gx_out = 0\n",
    "    gy_out = 0\n",
    "    gx_in = 0\n",
    "    gy_in = 0\n",
    "    for shape_point_count in range(shape.num_parts):\n",
    "        shape_point = shape.part(shape_point_count)\n",
    "        #print(\"顔器官No.{} 座標位置: ({},{})\".format(shape_point_count, shape_point.x, shape_point.y))\n",
    "        #器官ごとに描画\n",
    "        if shape_point_count<num_of_points_out:\n",
    "            cv2.circle(img,(shape_point.x, shape_point.y),circle_r,color_l_out, line_w)\n",
    "            gx_out = gx_out + shape_point.x/num_of_points_out\n",
    "            gy_out = gy_out + shape_point.y/num_of_points_out\n",
    "        else:\n",
    "            cv2.circle(img,(shape_point.x, shape_point.y),circle_r,color_l_in, line_w)\n",
    "            gx_in = gx_in + shape_point.x/num_of_points_in\n",
    "            gy_in = gy_in + shape_point.y/num_of_points_in\n",
    "\n",
    "    # 重心位置を描画\n",
    "    cv2.circle(img,(int(gx_out), int(gy_out)),circle_r,(0,0,255), line_w)\n",
    "    cv2.circle(img,(int(gx_in), int(gy_in)),circle_r,(0,0,0), line_w)\n",
    "\n",
    "    # 顔の方位を計算\n",
    "    theta = math.asin(2*(gx_in-gx_out)/(d.right()-d.left()))\n",
    "    radian = theta*180/math.pi\n",
    "    print(\"顔方位:{} (角度:{}度)\".format(theta,radian))\n",
    "\n",
    "    # 顔方位を表示\n",
    "    if radian<0:\n",
    "        textPrefix = \"   left \"\n",
    "    else:\n",
    "        textPrefix = \"   right \"\n",
    "    textShow = textPrefix + str(round(abs(radian),1)) + \" deg.\"\n",
    "    cv2.putText(img, textShow, (d.left(), d.top()), fontType, fontSize, color_f, line_w)\n",
    "\n",
    "\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.imwrite(\"temp.jpg\",img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDAk9Qhoij3p"
   },
   "source": [
    "### ノック８７：検出した情報を統合し、タイムラプスを作ってみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qoZRNzMUij3p",
    "outputId": "b1488dc0-84ab-4986-aba2-9d4b969b825c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タイムラプス生成を開始します\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'detectMultiScale'\n> Overload resolution failed:\n>  - Argument 'hitThreshold' can not be treated as a double\n>  - Argument 'hitThreshold' can not be treated as a double\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     31\u001b[0m     gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 32\u001b[0m     human, r \u001b[38;5;241m=\u001b[39m \u001b[43mhog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhogParams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(human)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m human:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'detectMultiScale'\n> Overload resolution failed:\n>  - Argument 'hitThreshold' can not be treated as a double\n>  - Argument 'hitThreshold' can not be treated as a double\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# タイムラプス: 数フレームから1フレームのみを取り出した早送り動画\n",
    "# 目で動画の傾向を簡単に掴むのに適している\n",
    "# デモ映像としても利用でき、分析結果に対する説得力を高めることにも効果的\n",
    "print(\"タイムラプス生成を開始します\")\n",
    "\n",
    "# 映像取得 #\n",
    "cap = cv2.VideoCapture(\"mov/mov01.avi\")\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# hog宣言 #\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0, 'finalThreshold':5}\n",
    "\n",
    "# タイムラプス作成 #\n",
    "movie_name = \"timelapse.avi\"\n",
    "# VideoWriter_fourcc: 動画ファイル作成\n",
    "# FourCCと呼ばれる動画のデータフォーマットを識別する四文字を指定し、任意の形式の動画ファイルを作成する\n",
    "# 下記はAVI形式\n",
    "fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')\n",
    "video = cv2.VideoWriter(movie_name,fourcc, 30, (width,height))\n",
    "\n",
    "num = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # 10フレームごとに保存\n",
    "        if (num % 10 == 0):\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            human, r = hog.detectMultiScale(gray, **hogParams)\n",
    "            if (len(human)>0):\n",
    "                for (x, y, w, h) in human:\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (255,255,255), 3)\n",
    "            # フレームを格納\n",
    "            video.write(frame)\n",
    "    else:\n",
    "        break\n",
    "    num = num + 1\n",
    "video.release()\n",
    "# 動画の生成\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"タイムラプス生成を終了しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWzwvWlIij3p"
   },
   "source": [
    "### ノック８８：全体像をグラフにして可視化してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "quHa70kuij3p",
    "outputId": "c6555a64-8c89-4d38-f7a9-6b5cb3aa5b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分析を開始します\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'detectMultiScale'\n> Overload resolution failed:\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     20\u001b[0m     gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 21\u001b[0m     human, r \u001b[38;5;241m=\u001b[39m hog\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhogParams)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(human)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m human:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'detectMultiScale'\n> Overload resolution failed:\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "print(\"分析を開始します\")\n",
    "# 映像取得 #\n",
    "cap = cv2.VideoCapture(\"mov/mov01.avi\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# hog宣言 #\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0, 'finalThreshold':5}\n",
    "\n",
    "num = 0\n",
    "list_df = pd.DataFrame( columns=['time','people'] )\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        if (num % 10 == 0):\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            human, r = hog.detectMultiScale(gray, **hogParams)\n",
    "            if (len(human)>0):\n",
    "                for (x, y, w, h) in human:\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (255,255,255), 3)\n",
    "            # フレーム番号/fps(=秒数), ヒトの数\n",
    "            tmp_se = pd.Series([num / fps, len(human)], index=list_df.columns)\n",
    "            list_df = list_df.append( tmp_se, ignore_index=True )       \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "    num = num + 1\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"分析を終了しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ipMR8ldcij3q",
    "outputId": "25daa8b7-0838-4e0d-dea6-8479c8256568"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 時間経過による人数の変化\n",
    "plt.plot(list_df[\"time\"], list_df[\"people\"])\n",
    "plt.xlabel('time(sec.)')\n",
    "plt.ylabel('population')\n",
    "plt.ylim(0,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkLDn5npij3q"
   },
   "source": [
    "### ノック８９：人通りの変化をグラフで確認しよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bPA2yN7Lij3q",
    "outputId": "20318362-c07a-4dd8-fbee-c62d764feab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分析を開始します\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'detectMultiScale'\n> Overload resolution failed:\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (num\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     20\u001b[0m     gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 21\u001b[0m     human, r \u001b[38;5;241m=\u001b[39m hog\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhogParams)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(human)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m human:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'detectMultiScale'\n> Overload resolution failed:\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n>  - 'finalThreshold' is an invalid keyword argument for HOGDescriptor.detectMultiScale()\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "print(\"分析を開始します\")\n",
    "# 映像取得 #\n",
    "cap = cv2.VideoCapture(\"mov/mov02.avi\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# hog宣言 #\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0, 'finalThreshold':5}\n",
    "\n",
    "num = 0\n",
    "list_df2 = pd.DataFrame( columns=['time','people'] )\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        if (num%10==0):\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            human, r = hog.detectMultiScale(gray, **hogParams)\n",
    "            if (len(human)>0):\n",
    "                for (x, y, w, h) in human:\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (255,255,255), 3)\n",
    "            tmp_se = pd.Series( [num/fps,len(human) ], index=list_df.columns )\n",
    "            list_df2 = list_df2.append( tmp_se, ignore_index=True )       \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "    num = num + 1\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"分析を終了しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYrUWgifij3q",
    "outputId": "d1299d47-b978-49fd-86f3-9d0c8072f524"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list_df2[\"time\"], list_df2[\"people\"])\n",
    "plt.xlabel('time(sec.)')\n",
    "plt.ylabel('population')\n",
    "plt.ylim(0,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-psXWcHij3r"
   },
   "source": [
    "### ノック９０：移動平均を計算することでノイズの影響を除去しよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "njAD_u10ij3r"
   },
   "outputs": [],
   "source": [
    "# hogで分析したデータはノイズが多い\n",
    "# ある程度の時間の平均(移動平均)を計算することでノイズの影響を軽減する\n",
    "\n",
    "import numpy as np\n",
    "def moving_average(x, y):\n",
    "    # np.convolve: 畳み込み積分,移動平均\n",
    "    y_conv = np.convolve(y, np.ones(5)/float(5), mode='valid')\n",
    "    # np.convolve: 等差数列を作成する関数(開始,終了,数)\n",
    "    x_dat = np.linspace(np.min(x), np.max(x), np.size(y_conv))\n",
    "    return x_dat, y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UJ68NS41ij3r",
    "outputId": "cca0a0f3-126d-4cc9-a489-da0e8aed6039"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "v cannot be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(list_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m], list_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeople\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m ma_x, ma_y \u001b[38;5;241m=\u001b[39m \u001b[43mmoving_average\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpeople\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(ma_x,ma_y, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime(sec.)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mmoving_average\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmoving_average\u001b[39m(x, y):\n\u001b[1;32m----> 5\u001b[0m     y_conv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     x_dat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(np\u001b[38;5;241m.\u001b[39mmin(x), np\u001b[38;5;241m.\u001b[39mmax(x), np\u001b[38;5;241m.\u001b[39msize(y_conv))\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_dat, y_conv\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconvolve\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:843\u001b[0m, in \u001b[0;36mconvolve\u001b[1;34m(a, v, mode)\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma cannot be empty\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv cannot be empty\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m multiarray\u001b[38;5;241m.\u001b[39mcorrelate(a, v[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], mode)\n",
      "\u001b[1;31mValueError\u001b[0m: v cannot be empty"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list_df[\"time\"], list_df[\"people\"], label=\"raw\")\n",
    "ma_x, ma_y = moving_average(list_df[\"time\"], list_df[\"people\"])\n",
    "plt.plot(ma_x,ma_y, label=\"average\")\n",
    "plt.xlabel('time(sec.)')\n",
    "plt.ylabel('population')\n",
    "plt.ylim(0,15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uvDXfGAij3r",
    "outputId": "f23757cf-d0bd-4eaa-abdc-2f5f274ea06a"
   },
   "outputs": [],
   "source": [
    "plt.plot(list_df2[\"time\"], list_df2[\"people\"], label=\"raw\")\n",
    "ma_x2, ma_y2 = moving_average(list_df2[\"time\"], list_df2[\"people\"])\n",
    "plt.plot(ma_x2,ma_y2, label=\"average\")\n",
    "plt.xlabel('time(sec.)')\n",
    "plt.ylabel('population')\n",
    "plt.ylim(0,15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKWlkYN1ij3r",
    "outputId": "e0c80e32-1ece-4050-8717-1bc8c8783f8f"
   },
   "outputs": [],
   "source": [
    "plt.plot(ma_x,ma_y, label=\"1st\")\n",
    "plt.plot(ma_x2,ma_y2, label=\"2nd\")\n",
    "plt.xlabel('time(sec.)')\n",
    "plt.ylabel('population')\n",
    "plt.ylim(0,15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EnxdBp8Pij3l",
    "2RXHi1jEij3m",
    "u33mA7sUij3n",
    "R0v2owrYij3n",
    "pK99r_Rjij3n"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
